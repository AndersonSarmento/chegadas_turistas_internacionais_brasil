{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bf14b226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession inicializada com sucesso!\n",
      "<pyspark.sql.session.SparkSession object at 0x78dc97567ec0>\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "import datetime # Para gerar timestamps no nome do arquivo de schema\n",
    "\n",
    "# Inicializa a SparkSession\n",
    "# O .builder fornece uma interface para configurar o Spark.\n",
    "# .appName() define um nome para sua aplicação Spark (útil para monitoramento).\n",
    "# .getOrCreate() tenta obter uma SparkSession existente ou cria uma nova se não houver.\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MeuPrimeiroSparkSession\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Opcional: Para verificar se a sessão foi criada, você pode imprimir o objeto spark\n",
    "print(\"SparkSession inicializada com sucesso!\")\n",
    "print(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7a5f10ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "# Removido: from pyspark.sql import SparkSession, pois a sessão 'spark' será global\n",
    "\n",
    "def read_csv(caminho_do_arquivo_csv: str):\n",
    "    \"\"\"\n",
    "    Lê um arquivo CSV específico usando PySpark e retorna o DataFrame.\n",
    "    Assume que a SparkSession (variável 'spark') já está inicializada e disponível\n",
    "    no escopo global ou acessível.\n",
    "    \n",
    "    Os parâmetros 'delimiter' e 'charset' são definidos internamente.\n",
    "\n",
    "    Args:\n",
    "        caminho_do_arquivo_csv (str): O caminho completo para o arquivo CSV a ser lido.\n",
    "                                     Ex: \"/home/ander/Documentos/projetos/chegadas_turistas_internacionais_brasil/data/chegadas_1989.csv\"\n",
    "\n",
    "    Returns:\n",
    "        pyspark.sql.DataFrame: O DataFrame contendo os dados do CSV, ou None em caso de erro.\n",
    "    \"\"\"\n",
    "    # Parâmetros fixos para a leitura do CSV\n",
    "    DELIMITER = \";\"\n",
    "    CHARSET = \"windows-1252\"\n",
    "\n",
    "    print(f\"--- Iniciando leitura do CSV: {os.path.basename(caminho_do_arquivo_csv)} ---\")\n",
    "\n",
    "    try:\n",
    "        df = spark.read.format(\"csv\") \\\n",
    "          .option(\"header\", \"true\") \\\n",
    "          .option(\"inferSchema\", \"true\") \\\n",
    "          .option(\"delimiter\", DELIMITER) \\\n",
    "          .option(\"charset\", CHARSET) \\\n",
    "          .load(caminho_do_arquivo_csv)\n",
    "\n",
    "        print(f\"CSV '{os.path.basename(caminho_do_arquivo_csv)}' lido com sucesso.\")\n",
    "        return df\n",
    "\n",
    "    except NameError:\n",
    "        print(\"Erro: A variável 'spark' (SparkSession) não está definida. Certifique-se de inicializá-la antes de chamar esta função.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao ler o CSV '{caminho_do_arquivo_csv}': {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1a3179d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.types import (\n",
    "    StringType, IntegerType, LongType, FloatType, DoubleType,\n",
    "    BooleanType, DateType, TimestampType, StructField\n",
    ")\n",
    "\n",
    "# --- 3. A nova função para gerar e salvar o CREATE TABLE MySQL ---\n",
    "def gerar_e_salvar_create_table_mysql(df: DataFrame, table_name: str, output_dir: str):\n",
    "    \"\"\"\n",
    "    Gera um script SQL 'CREATE TABLE' para MySQL a partir do schema de um DataFrame PySpark\n",
    "    e o salva em um arquivo.\n",
    "    \"\"\"\n",
    "    mysql_types_map = {\n",
    "        StringType: \"VARCHAR(255)\",\n",
    "        IntegerType: \"INT\",\n",
    "        LongType: \"BIGINT\",\n",
    "        FloatType: \"FLOAT\",\n",
    "        DoubleType: \"DOUBLE\",\n",
    "        BooleanType: \"BOOLEAN\",\n",
    "        DateType: \"DATE\",\n",
    "        TimestampType: \"DATETIME\",\n",
    "    }\n",
    "    print(f\"--- Gerando script CREATE TABLE para '{table_name}' ---\")\n",
    "    columns = []\n",
    "    for field in df.schema.fields:\n",
    "        col_name = field.name\n",
    "        col_type = type(field.dataType)\n",
    "        sql_type = mysql_types_map.get(col_type)\n",
    "        if sql_type is None:\n",
    "            print(f\"Aviso: Tipo PySpark '{field.dataType}' para a coluna '{col_name}' não mapeado, usando VARCHAR(255).\")\n",
    "            sql_type = \"VARCHAR(255)\"\n",
    "        columns.append(f\"`{col_name}` {sql_type}\")\n",
    "    \n",
    "    columns_sql = \",\\n    \".join(columns)\n",
    "    create_table_sql = f\"CREATE TABLE IF NOT EXISTS `{table_name}` (\\n    {columns_sql}\\n);\"\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    sql_file_path = os.path.join(output_dir, f\"{table_name}_create_table.sql\")\n",
    "    \n",
    "    try:\n",
    "        with open(sql_file_path, \"w\") as sql_file:\n",
    "            sql_file.write(create_table_sql)\n",
    "        print(f\"Script CREATE TABLE salvo com sucesso em: {sql_file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao salvar o script SQL para '{table_name}': {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1743ca81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Iniciando leitura do CSV: chegadas_1989.csv ---\n",
      "CSV 'chegadas_1989.csv' lido com sucesso.\n",
      "\n",
      "CSV lido com sucesso. Gerando script SQL...\n",
      "--- Gerando script CREATE TABLE para 'chegadas_turistas_1989' ---\n",
      "Script CREATE TABLE salvo com sucesso em: /home/ander/Documentos/projetos/chegadas_turistas_internacionais_brasil/schemas/chegadas_turistas_1989_create_table.sql\n"
     ]
    }
   ],
   "source": [
    "# --- Exemplo de uso no seu fluxo ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Caminho do seu CSV (como usuário Ubuntu, você está acostumado com essa estrutura)\n",
    "    caminho_csv = \"/home/ander/Documentos/projetos/chegadas_turistas_internacionais_brasil/data/chegadas_1989.csv\"\n",
    "    \n",
    "    # Diretório para salvar o script SQL de criação da tabela\n",
    "    diretorio_sql_output = \"/home/ander/Documentos/projetos/chegadas_turistas_internacionais_brasil/schemas\"\n",
    "    \n",
    "    # Nome da tabela no MySQL\n",
    "    nome_da_tabela = \"chegadas_turistas_1989\" # Pode ser dinâmico também\n",
    "\n",
    "    # 1. Ler o CSV com PySpark\n",
    "    df_chegadas = read_csv(caminho_csv)\n",
    "\n",
    "    if df_chegadas is not None:\n",
    "        print(\"\\nCSV lido com sucesso. Gerando script SQL...\")\n",
    "        # 2. Gerar e salvar o script CREATE TABLE para MySQL\n",
    "        gerar_e_salvar_create_table_mysql(df_chegadas, nome_da_tabela, diretorio_sql_output)\n",
    "    else:\n",
    "        print(\"\\nNão foi possível ler o CSV, pulando a geração do script SQL.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ac3b13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chegadas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
